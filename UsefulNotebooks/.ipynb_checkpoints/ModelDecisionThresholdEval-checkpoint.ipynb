{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models for Alternative Decision Thresholds (for AutoMLPipe-BC)\n",
    "Allows users to specify alternative decision thresholds (rather than the standard .5 probability of case) and re-evaluate algorithm performane metrics \n",
    "\n",
    "Designed to operate following application of pipeline phases 1-6. Relies on folder/file hierarchy saved by the pipeline.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mean,stdev\n",
    "import numpy as np\n",
    "from scipy import interp,stats\n",
    "#Evalutation metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_path = \"C:/Users/ryanu/Documents/Analysis/AutoMLPipe_Experiments/hcc_demo\"\n",
    "experiment_path = \"C:/Users/ryanurb/Documents/Analysis/AutoMLPipe_Experiments/hcc_demo\"\n",
    "targetDataName = 'None' # 'None' if user wants to generate visualizations for all analyzed datasets\n",
    "algorithms = [] #use empty list if user wishes re-evaluate all modeling algorithms that were run in pipeline.\n",
    "threshold = 0.3 # Threshold of case probability used to predict case (typically 0.5 by default in modeling)\n",
    "#available_algorithms = ['Naive Bayes','Logistic Regression','Decision Tree','Random Forest','Gradient Boosting','XGB','LGB','SVM','ANN','K Neighbors','eLCS','XCS','ExSTraCS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically detect data folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed Datasets: ['hcc-data_example', 'hcc-data_example_no_covariates']\n"
     ]
    }
   ],
   "source": [
    "# Get dataset paths for all completed dataset analyses in experiment folder\n",
    "datasets = os.listdir(experiment_path)\n",
    "experiment_name = experiment_path.split('/')[-1] #Name of experiment folder\n",
    "datasets.remove('metadata.csv')\n",
    "datasets.remove('jobsCompleted')\n",
    "try:\n",
    "    datasets.remove('logs')\n",
    "    datasets.remove('jobs')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    datasets.remove('DatasetComparisons') #If it has been run previously (overwrite)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    datasets.remove('KeyFileCopy') #If it has been run previously (overwrite)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    datasets.remove(experiment_name+'_ML_Pipeline_Report.pdf') #If it has been run previously (overwrite)\n",
    "except:\n",
    "    pass\n",
    "datasets = sorted(datasets) #ensures consistent ordering of datasets\n",
    "print(\"Analyzed Datasets: \"+str(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load other necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load variables specified earlier in the pipeline from metadata file\n",
    "jupyterRun = 'True'\n",
    "metadata = pd.read_csv(experiment_path + '/' + 'metadata.csv').values\n",
    "class_label = metadata[0, 1]\n",
    "instance_label = metadata[1, 1]\n",
    "cv_partitions = int(metadata[6,1])\n",
    "do_NB = metadata[19,1]\n",
    "do_LR = metadata[20,1]\n",
    "do_DT = metadata[21,1]\n",
    "do_RF = metadata[22,1]\n",
    "do_GB = metadata[23, 1]\n",
    "do_XGB = metadata[24,1]\n",
    "do_LGB = metadata[25,1]\n",
    "do_SVM = metadata[26,1]\n",
    "do_ANN = metadata[27,1]\n",
    "do_KN = metadata[28, 1]\n",
    "do_eLCS = metadata[29,1]\n",
    "do_XCS = metadata[30,1]\n",
    "do_ExSTraCS = metadata[31,1]\n",
    "primary_metric = metadata[32,1]\n",
    "\n",
    "possible_algos = ['Naive Bayes','Logistic Regression','Decision Tree','Random Forest','Gradient Boosting','XGB','LGB','SVM','ANN','K Neighbors','eLCS','XCS','ExSTraCS']\n",
    "abbrev = {'Naive Bayes':'NB','Logistic Regression':'LR','Decision Tree':'DT','Random Forest':'RF','Gradient Boosting':'GB','XGB':'XGB','LGB':'LGB','SVM':'SVM','ANN':'ANN','K Neighbors':'KN','eLCS':'eLCS','XCS':'XCS','ExSTraCS':'ExSTraCS'}\n",
    "colors = {'Naive Bayes':'grey','Logistic Regression':'black','Decision Tree':'yellow','Random Forest':'orange','Gradient Boosting':'bisque','XGB':'purple','LGB':'aqua','SVM':'blue','ANN':'red','eLCS':'firebrick','XCS':'deepskyblue','K Neighbors':'seagreen','ExSTraCS':'lightcoral'}\n",
    "\n",
    "#Create algorithms list (i.e. modeling algorithms that were run in the pipeline)\n",
    "if eval(do_NB):\n",
    "    algorithms.append('Naive Bayes')\n",
    "if eval(do_LR):\n",
    "    algorithms.append('Logistic Regression')\n",
    "if eval(do_DT):\n",
    "    algorithms.append('Decision Tree')\n",
    "if eval(do_RF):\n",
    "    algorithms.append('Random Forest')\n",
    "if eval(do_GB):\n",
    "    algorithms.append('Gradient Boosting')\n",
    "if eval(do_XGB):\n",
    "    algorithms.append('XGB')\n",
    "if eval(do_LGB):\n",
    "    algorithms.append('LGB')\n",
    "if eval(do_SVM):\n",
    "    algorithms.append('SVM')\n",
    "if eval(do_ANN):\n",
    "    algorithms.append('ANN')\n",
    "if eval(do_KN):\n",
    "    algorithms.append('K Neighbors')\n",
    "if eval(do_eLCS):\n",
    "    algorithms.append('eLCS')\n",
    "if eval(do_XCS):\n",
    "    algorithms.append('XCS')\n",
    "if eval(do_ExSTraCS):\n",
    "    algorithms.append('ExSTraCS')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define necessary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classEval(y_true, y_pred):\n",
    "    \"\"\" Calculates standard classification metrics including:\n",
    "    True positives, false positives, true negative, false negatives, standard accuracy, balanced accuracy\n",
    "    recall, precision, f1 score, negative predictive value, likelihood ratio positive, and likelihood ratio negative\"\"\"\n",
    "    #Calculate true positive, true negative, false positive, and false negative.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    #Calculate Accuracy metrics\n",
    "    ac = accuracy_score(y_true, y_pred)\n",
    "    bac = balanced_accuracy_score(y_true, y_pred)\n",
    "    #Calculate Precision and Recall\n",
    "    re = recall_score(y_true, y_pred)\n",
    "    pr = precision_score(y_true, y_pred)\n",
    "    #Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # Calculate specificity\n",
    "    if tn == 0 and fp == 0:\n",
    "        sp = 0\n",
    "    else:\n",
    "        sp = tn / float(tn + fp)\n",
    "    # Calculate Negative predictive value\n",
    "    if tn == 0 and fn == 0:\n",
    "        npv = 0\n",
    "    else:\n",
    "        npv = tn/float(tn+fn)\n",
    "    # Calculate likelihood ratio postive\n",
    "    if sp == 1:\n",
    "        lrp = 0\n",
    "    else:\n",
    "        lrp = re/float(1-sp)\n",
    "    # Calculate likeliehood ratio negative\n",
    "    if sp == 0:\n",
    "        lrm = 0\n",
    "    else:\n",
    "        lrm = (1-re)/float(sp)\n",
    "    return [bac, ac, f1, re, sp, pr, tp, tn, fp, fn, npv, lrp, lrm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primaryStats(algorithms,original_headers,cv_partitions,full_path,data_name,instance_label,class_label,abbrev,colors,plot_ROC,plot_PRC,jupyterRun,name_modifier,legend_inside_plot):\n",
    "    \"\"\" Combine classification metrics and model feature importance scores\"\"\"\n",
    "    result_table = []\n",
    "    metric_dict = {}\n",
    "    for algorithm in algorithms: #completed for each individual ML modeling algorithm\n",
    "        alg_result_table = [] #stores values used in ROC and PRC plots\n",
    "        # Define evaluation stats variable lists\n",
    "        s_bac = [] # balanced accuracies\n",
    "        s_ac = [] # standard accuracies\n",
    "        s_f1 = [] # F1 scores\n",
    "        s_re = [] # recall values\n",
    "        s_sp = [] # specificities\n",
    "        s_pr = [] # precision values\n",
    "        s_tp = [] # true positives\n",
    "        s_tn = [] # true negatives\n",
    "        s_fp = [] # false positives\n",
    "        s_fn = [] # false negatives\n",
    "        s_npv = [] # negative predictive values\n",
    "        s_lrp = [] # likelihood ratio positive values\n",
    "        s_lrm = [] # likelihood ratio negative values\n",
    "        # Define ROC plot variable lists\n",
    "        tprs = [] # true postitive rates\n",
    "        aucs = [] #areas under ROC curve\n",
    "        mean_fpr = np.linspace(0, 1, 100) #used to plot all CVs in single ROC plot\n",
    "        mean_recall = np.linspace(0, 1, 100) #used to plot all CVs in single PRC plot\n",
    "        # Define PRC plot variable lists\n",
    "        precs = [] #precision values for PRC\n",
    "        praucs = [] #area under PRC curve\n",
    "        aveprecs = [] #average precisions for PRC\n",
    "        \n",
    "        #Gather statistics over all CV partitions\n",
    "        for cvCount in range(0,cv_partitions):\n",
    "            #Unpickle saved metrics from previous phase\n",
    "            result_file = full_path+'/training/'+abbrev[algorithm]+\"_CV_\"+str(cvCount)+\"_metrics\"\n",
    "            file = open(result_file, 'rb')\n",
    "            results = pickle.load(file)\n",
    "            file.close()\n",
    "            #Separate pickled results\n",
    "            metricList = results[0]\n",
    "            fpr = results[1]\n",
    "            tpr = results[2]\n",
    "            roc_auc = results[3]\n",
    "            prec = results[4]\n",
    "            recall = results[5]\n",
    "            prec_rec_auc = results[6]\n",
    "            ave_prec = results[7]\n",
    "            #Separate metrics from metricList\n",
    "            s_bac.append(metricList[0])\n",
    "            s_ac.append(metricList[1])\n",
    "            s_f1.append(metricList[2])\n",
    "            s_re.append(metricList[3])\n",
    "            s_sp.append(metricList[4])\n",
    "            s_pr.append(metricList[5])\n",
    "            s_tp.append(metricList[6])\n",
    "            s_tn.append(metricList[7])\n",
    "            s_fp.append(metricList[8])\n",
    "            s_fn.append(metricList[9])\n",
    "            s_npv.append(metricList[10])\n",
    "            s_lrp.append(metricList[11])\n",
    "            s_lrm.append(metricList[12])\n",
    "            #update list that stores values used in ROC and PRC plots\n",
    "            alg_result_table.append([fpr, tpr, roc_auc, recall, prec, prec_rec_auc, ave_prec])\n",
    "            # Update ROC plot variable lists needed to plot all CVs in one ROC plot\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            aucs.append(roc_auc)\n",
    "            # Update PRC plot variable lists needed to plot all CVs in one PRC plot\n",
    "            precs.append(interp(mean_recall, recall, prec))\n",
    "            praucs.append(prec_rec_auc)\n",
    "            aveprecs.append(ave_prec)\n",
    "            \n",
    "        if jupyterRun:\n",
    "            print(algorithm)\n",
    "        #Define values for the mean ROC line (mean of individual CVs)\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = np.mean(aucs)\n",
    "        #Generate ROC Plot (including individual CV's lines, average line, and no skill line) - based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html-----------------------\n",
    "        if eval(plot_ROC):\n",
    "            # Set figure dimensions\n",
    "            plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "            # Plot individual CV ROC lines\n",
    "            for i in range(cv_partitions):\n",
    "                plt.plot(alg_result_table[i][0], alg_result_table[i][1], lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.3f)' % (i, alg_result_table[i][2]))\n",
    "            # Plot no-skill line\n",
    "            plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='No-Skill', alpha=.8)\n",
    "            # Plot average line for all CVs\n",
    "            std_auc = np.std(aucs) # AUC standard deviations across CVs\n",
    "            plt.plot(mean_fpr, mean_tpr, color=colors[algorithm],label=r'Mean ROC (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n",
    "            # Plot standard deviation grey zone of curves\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "            #Specify plot axes,labels, and legend\n",
    "            plt.xlim([-0.05, 1.05])\n",
    "            plt.ylim([-0.05, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            if legend_inside_plot:\n",
    "                plt.legend(loc=\"lower right\")\n",
    "            else:\n",
    "                plt.legend(loc=\"upper left\", bbox_to_anchor=(1.01,1))\n",
    "            #Export and/or show plot\n",
    "            plt.savefig(full_path+'/training/results/'+abbrev[algorithm]+\"_ROC\"+name_modifier+\".png\", bbox_inches=\"tight\")\n",
    "            if eval(jupyterRun):\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close('all')\n",
    "\n",
    "        #Define values for the mean PRC line (mean of individual CVs)\n",
    "        mean_prec = np.mean(precs, axis=0)\n",
    "        mean_pr_auc = np.mean(praucs)\n",
    "        #Generate PRC Plot (including individual CV's lines, average line, and no skill line)------------------------------------------------------------------------------------------------------------------\n",
    "        if eval(plot_PRC):\n",
    "            # Set figure dimensions\n",
    "            plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "            # Plot individual CV PRC lines\n",
    "            for i in range(cv_partitions):\n",
    "                plt.plot(alg_result_table[i][3], alg_result_table[i][4], lw=1, alpha=0.3, label='PRC fold %d (AUC = %0.3f)' % (i, alg_result_table[i][5]))\n",
    "            #Estimate no skill line based on the fraction of cases found in the first test dataset\n",
    "            test = pd.read_csv(full_path + '/CVDatasets/' + data_name + '_CV_0_Test.csv') #Technically there could be a unique no-skill line for each CV dataset based on final class balance (however only one is needed, and stratified CV attempts to keep partitions with similar/same class balance)\n",
    "            testY = test[class_label].values\n",
    "            noskill = len(testY[testY == 1]) / len(testY)  # Fraction of cases\n",
    "            # Plot no-skill line\n",
    "            plt.plot([0, 1], [noskill, noskill], color='orange', linestyle='--', label='No-Skill', alpha=.8)\n",
    "            # Plot average line for all CVs\n",
    "            std_pr_auc = np.std(praucs)\n",
    "            # Plot standard deviation grey zone of curves\n",
    "            plt.plot(mean_recall, mean_prec, color=colors[algorithm],label=r'Mean PRC (AUC = %0.3f $\\pm$ %0.3f)' % (mean_pr_auc, std_pr_auc),lw=2, alpha=.8)\n",
    "            std_prec = np.std(precs, axis=0)\n",
    "            precs_upper = np.minimum(mean_prec + std_prec, 1)\n",
    "            precs_lower = np.maximum(mean_prec - std_prec, 0)\n",
    "            plt.fill_between(mean_fpr, precs_lower, precs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n",
    "            #Specify plot axes,labels, and legend\n",
    "            plt.xlim([-0.05, 1.05])\n",
    "            plt.ylim([-0.05, 1.05])\n",
    "            plt.xlabel('Recall (Sensitivity)')\n",
    "            plt.ylabel('Precision (PPV)')\n",
    "            if legend_inside_plot:\n",
    "                plt.legend(loc=\"upper right\")\n",
    "            else:\n",
    "                plt.legend(loc=\"upper left\", bbox_to_anchor=(1.01,1))\n",
    "            #Export and/or show plot\n",
    "            plt.savefig(full_path+'/training/results/'+abbrev[algorithm]+\"_PRC\"+name_modifier+\".png\", bbox_inches=\"tight\")\n",
    "            if eval(jupyterRun):\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close('all')\n",
    "                \n",
    "        #Export and save all CV metric stats for each individual algorithm  -----------------------------------------------------------------------------\n",
    "        results = {'Balanced Accuracy': s_bac, 'Accuracy': s_ac, 'F1_Score': s_f1, 'Sensitivity (Recall)': s_re, 'Specificity': s_sp,'Precision (PPV)': s_pr, 'TP': s_tp, 'TN': s_tn, 'FP': s_fp, 'FN': s_fn, 'NPV': s_npv, 'LR+': s_lrp, 'LR-': s_lrm, 'ROC_AUC': aucs,'PRC_AUC': praucs, 'PRC_APS': aveprecs}\n",
    "        metric_dict[algorithm] = results\n",
    "\n",
    "        #Store ave metrics for creating global ROC and PRC plots later\n",
    "        mean_ave_prec = np.mean(aveprecs)\n",
    "        result_dict = {'algorithm':algorithm,'fpr':mean_fpr, 'tpr':mean_tpr, 'auc':mean_auc, 'prec':mean_prec, 'pr_auc':mean_pr_auc, 'ave_prec':mean_ave_prec}\n",
    "        result_table.append(result_dict)\n",
    "    #Result table later used to create global ROC an PRC plots comparing average ML algorithm performance.\n",
    "    result_table = pd.DataFrame.from_dict(result_table)\n",
    "    result_table.set_index('algorithm',inplace=True)\n",
    "\n",
    "    return result_table,metric_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ROC and PRC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "hcc-data_example\n",
      "---------------------------------------\n",
      "Naive Bayes\n",
      "0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1.]\n",
      "[[9.97406489e-01 2.59351142e-03]\n",
      " [9.99102336e-01 8.97663652e-04]\n",
      " [9.99999191e-01 8.08525595e-07]\n",
      " [1.00000000e+00 2.03323992e-15]\n",
      " [9.99999998e-01 1.77195505e-09]\n",
      " [9.99999973e-01 2.66918833e-08]\n",
      " [9.99225002e-01 7.74997624e-04]\n",
      " [9.99180203e-01 8.19797076e-04]\n",
      " [9.99999982e-01 1.76963061e-08]\n",
      " [9.99998485e-01 1.51478647e-06]\n",
      " [9.99631896e-01 3.68103723e-04]\n",
      " [9.99999998e-01 1.55873726e-09]\n",
      " [9.99999061e-01 9.39434296e-07]\n",
      " [9.99999989e-01 1.14043636e-08]\n",
      " [9.99999997e-01 3.44649330e-09]\n",
      " [9.99999999e-01 1.27951549e-09]\n",
      " [9.99999991e-01 8.67450989e-09]\n",
      " [9.99999999e-01 5.01298309e-10]\n",
      " [9.99999740e-01 2.59643505e-07]\n",
      " [9.99999889e-01 1.11343288e-07]\n",
      " [9.99999995e-01 4.91032464e-09]\n",
      " [9.99711362e-01 2.88637603e-04]\n",
      " [9.99999991e-01 8.52547617e-09]\n",
      " [9.99999794e-01 2.05690761e-07]\n",
      " [1.00000000e+00 1.13668154e-10]\n",
      " [9.99961373e-01 3.86274929e-05]\n",
      " [9.99999988e-01 1.17025301e-08]\n",
      " [9.99852902e-01 1.47097835e-04]\n",
      " [1.00000000e+00 1.84228092e-10]\n",
      " [1.00000000e+00 3.89146285e-10]\n",
      " [9.99999922e-01 7.76495892e-08]\n",
      " [1.00000000e+00 4.92162667e-10]\n",
      " [9.99999995e-01 4.52254534e-09]\n",
      " [1.24917201e-06 9.99998751e-01]\n",
      " [9.40402425e-01 5.95975747e-02]\n",
      " [4.89448380e-19 1.00000000e+00]\n",
      " [9.99999972e-01 2.76743044e-08]\n",
      " [9.99992945e-01 7.05549200e-06]\n",
      " [9.99999752e-01 2.47817867e-07]\n",
      " [9.99999972e-01 2.84425765e-08]\n",
      " [9.89606217e-01 1.03937834e-02]\n",
      " [3.58178050e-02 9.64182195e-01]\n",
      " [8.61832721e-01 1.38167279e-01]\n",
      " [1.83743641e-24 1.00000000e+00]\n",
      " [9.99998744e-01 1.25580549e-06]\n",
      " [9.99896408e-01 1.03591715e-04]\n",
      " [9.99990228e-01 9.77191136e-06]\n",
      " [1.00000000e+00 5.53866105e-14]\n",
      " [9.99990121e-01 9.87920697e-06]\n",
      " [9.99999766e-01 2.34244668e-07]\n",
      " [9.99983950e-01 1.60498275e-05]\n",
      " [9.99568404e-01 4.31595946e-04]\n",
      " [1.97078754e-02 9.80292125e-01]\n",
      " [5.79033521e-04 9.99420966e-01]\n",
      " [9.99997995e-01 2.00515575e-06]]\n",
      "[ True  True False False False False  True  True False  True  True False\n",
      " False False False False False False False False False  True False False\n",
      " False  True False  True False False False False False  True  True  True\n",
      " False  True False False  True  True  True  True  True  True  True False\n",
      "  True False  True  True  True  True  True]\n",
      "[1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0.7338935574229692, 0.7272727272727273, 0.6808510638297872, 0.7619047619047619, 0.7058823529411765, 0.6153846153846154, 16, 24, 10, 5, 0.8275862068965517, 2.5904761904761906, 0.3373015873015873]\n",
      "---------------------------------------\n",
      "hcc-data_example_no_covariates\n",
      "---------------------------------------\n",
      "Naive Bayes\n",
      "0\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1.]\n",
      "[[9.97787567e-01 2.21243277e-03]\n",
      " [9.99192958e-01 8.07041820e-04]\n",
      " [9.99997751e-01 2.24875944e-06]\n",
      " [1.00000000e+00 3.83661114e-15]\n",
      " [9.99999993e-01 6.72048388e-09]\n",
      " [9.99999974e-01 2.57482124e-08]\n",
      " [9.99295457e-01 7.04543371e-04]\n",
      " [9.98734637e-01 1.26536319e-03]\n",
      " [9.99999961e-01 3.91571182e-08]\n",
      " [9.99997813e-01 2.18690671e-06]\n",
      " [9.99770916e-01 2.29083799e-04]\n",
      " [9.99999999e-01 1.02314313e-09]\n",
      " [9.99998614e-01 1.38623667e-06]\n",
      " [9.99999984e-01 1.64007333e-08]\n",
      " [9.99999998e-01 1.83626694e-09]\n",
      " [9.99999999e-01 1.33367698e-09]\n",
      " [9.99999989e-01 1.07787739e-08]\n",
      " [9.99999998e-01 1.76547886e-09]\n",
      " [9.99999868e-01 1.32120384e-07]\n",
      " [9.99999821e-01 1.79060784e-07]\n",
      " [9.99999995e-01 4.70901221e-09]\n",
      " [9.99866892e-01 1.33107727e-04]\n",
      " [9.99999996e-01 4.10553506e-09]\n",
      " [9.99999858e-01 1.41915783e-07]\n",
      " [1.00000000e+00 1.41833711e-10]\n",
      " [9.99866027e-01 1.33972929e-04]\n",
      " [9.99999954e-01 4.60960396e-08]\n",
      " [9.99833731e-01 1.66268978e-04]\n",
      " [1.00000000e+00 4.27854961e-10]\n",
      " [9.99999999e-01 1.27516542e-09]\n",
      " [9.99999886e-01 1.14060527e-07]\n",
      " [1.00000000e+00 3.72161395e-10]\n",
      " [9.99999992e-01 8.32920734e-09]\n",
      " [3.73602282e-07 9.99999626e-01]\n",
      " [9.59787138e-01 4.02128621e-02]\n",
      " [4.33678113e-19 1.00000000e+00]\n",
      " [9.99999966e-01 3.43264134e-08]\n",
      " [9.99994353e-01 5.64694016e-06]\n",
      " [9.99999835e-01 1.65135875e-07]\n",
      " [9.99999932e-01 6.78270133e-08]\n",
      " [9.91930446e-01 8.06955386e-03]\n",
      " [5.87021708e-02 9.41297829e-01]\n",
      " [8.38189989e-01 1.61810011e-01]\n",
      " [1.31054909e-24 1.00000000e+00]\n",
      " [9.99996256e-01 3.74425188e-06]\n",
      " [9.99902449e-01 9.75506598e-05]\n",
      " [9.99950703e-01 4.92967597e-05]\n",
      " [1.00000000e+00 2.45078682e-13]\n",
      " [9.99973939e-01 2.60610234e-05]\n",
      " [9.99999691e-01 3.08870421e-07]\n",
      " [9.99968054e-01 3.19457713e-05]\n",
      " [9.99567201e-01 4.32798866e-04]\n",
      " [1.53136005e-02 9.84686399e-01]\n",
      " [3.60751586e-04 9.99639248e-01]\n",
      " [9.99997269e-01 2.73075414e-06]]\n",
      "[ True  True  True False False False  True  True False  True  True False\n",
      "  True False False False False False False False False  True False False\n",
      " False  True False  True False False False False False  True  True  True\n",
      " False  True False False  True  True  True  True  True  True  True False\n",
      "  True False  True  True  True  True  True]\n",
      "[1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0.7044817927170868, 0.6909090909090909, 0.6530612244897959, 0.7619047619047619, 0.6470588235294118, 0.5714285714285714, 16, 22, 12, 5, 0.8148148148148148, 2.1587301587301586, 0.367965367965368]\n"
     ]
    }
   ],
   "source": [
    "if not targetDataName == 'None': # User specified one analyzed dataset above (if more than one were analyzed)\n",
    "    for each in datasets:\n",
    "        if not each == targetDataName:\n",
    "            datasets.remove(each)\n",
    "    print(\"Vizualized Datasets: \"+str(datasets))\n",
    "\n",
    "for each in datasets: #each analyzed dataset to make plots for\n",
    "    print(\"---------------------------------------\")\n",
    "    print(each)\n",
    "    print(\"---------------------------------------\")\n",
    "    full_path = experiment_path+'/'+each\n",
    "    #Create folder for tree vizualization files\n",
    "    original_headers = pd.read_csv(full_path+\"/exploratory/OriginalHeaders.csv\",sep=',').columns.values.tolist() #Get Original Headers\n",
    "\n",
    "    for algorithm in algorithms: #loop through algorithms\n",
    "        print(algorithm)\n",
    "        for cvCount in range(0,cv_partitions): #loop through cv's\n",
    "            print(str(cvCount))\n",
    "            #load testing data\n",
    "            test_file_path = full_path + '/CVDatasets/' + each + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n",
    "            test = pd.read_csv(test_file_path)\n",
    "            testY = test[class_label].values\n",
    "            del test #memory cleanup\n",
    "            \n",
    "            #Load pickled metric file for given algorithm and cv\n",
    "            result_file = full_path+'/training/'+abbrev[algorithm]+\"_CV_\"+str(cvCount)+\"_metrics\"\n",
    "            file = open(result_file, 'rb')\n",
    "            results = pickle.load(file)\n",
    "            file.close()\n",
    "\n",
    "            #load probas_ for model file\n",
    "            probas_ = results[9]\n",
    "\n",
    "            \n",
    "            #Get new class predictions given specified decision threshold\n",
    "            y_pred = probas_[:,1] > 0.000001 #threshold\n",
    "\n",
    "            integer_map = map(int, y_pred) \n",
    "            integer_list = list(integer_map)\n",
    "\n",
    "\n",
    "            #Calculate standard classificaction metrics\n",
    "            metricList = classEval(testY, y_pred)\n",
    " \n",
    "            \n",
    "\n",
    "            #add to metric list\n",
    "\n",
    "            #create new cv metric file for each algorithm\n",
    "    \n",
    "    #create new average metric file \n",
    "    \n",
    "    \n",
    "    #result_table,metric_dict = primaryStats(algorithms,original_headers,cv_partitions,full_path,each,instance_label,class_label,abbrev,colors,plot_ROC,plot_PRC,jupyterRun,name_modifier,legend_inside_plot)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
